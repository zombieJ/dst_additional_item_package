{
  "pretrained_model_name_or_path": "D:\\AI\\AI\u7ed8\u56fe\\models\\Stable-diffusion\\chilloutmix_NiPrunedFp32Fix.safetensors",
  "v2": false,
  "v_parameterization": false,
  "logging_dir": "D:\\AI\\my_lora\\-\\log",
  "train_data_dir": "D:\\AI\\my_lora\\-\\image",
  "reg_data_dir": "",
  "output_dir": "D:\\AI\\my_lora\\-\\model",
  "max_resolution": "512,512",
  "learning_rate": "3e-6",
  "lr_scheduler": "cosine_with_restarts",
  "lr_warmup": "5",
  "train_batch_size": 1,
  "epoch": "6",
  "save_every_n_epochs": "1",
  "mixed_precision": "bf16",
  "save_precision": "bf16",
  "seed": "1234",
  "num_cpu_threads_per_process": 2,
  "cache_latents": true,
  "caption_extension": ".txt",
  "enable_bucket": true,
  "gradient_checkpointing": false,
  "full_fp16": false,
  "no_token_padding": false,
  "stop_text_encoder_training": 0,
  "use_8bit_adam": false,
  "xformers": true,
  "save_model_as": "safetensors",
  "shuffle_caption": false,
  "save_state": false,
  "resume": "",
  "prior_loss_weight": 1.0,
  "text_encoder_lr": "1e-5",
  "unet_lr": "5e-5",
  "network_dim": 128,
  "lora_network_weights": "",
  "color_aug": false,
  "flip_aug": false,
  "clip_skip": 2,
  "gradient_accumulation_steps": 1.0,
  "mem_eff_attn": false,
  "output_name": "-",
  "model_list": "custom",
  "max_token_length": "75",
  "max_train_epochs": "",
  "max_data_loader_n_workers": "",
  "network_alpha": 64,
  "training_comment": "",
  "keep_tokens": "0",
  "lr_scheduler_num_cycles": "",
  "lr_scheduler_power": "",
  "persistent_data_loader_workers": true,
  "bucket_no_upscale": true,
  "random_crop": false,
  "bucket_reso_steps": 64.0,
  "caption_dropout_every_n_epochs": 0.0,
  "caption_dropout_rate": 0,
  "optimizer": "Lion",
  "optimizer_args": "",
  "noise_offset": "",
  "LoRA_type": "Standard",
  "conv_dim": 1,
  "conv_alpha": 1
}